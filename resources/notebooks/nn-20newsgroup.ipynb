{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing twenty-newsgroup dataset\n",
    "the twenty-newsgorup datasets is a collection of news articles.  \n",
    "Each article is labeled according to its type for example : medical news, automobile news,...etc.  \n",
    "In this tutorial we will deal with 4 classes of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['rec.autos', 'comp.graphics', 'sci.med', 'sci.electronics']\n",
    "no_classes = len(categories)\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test',categories=categories,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF statistic\n",
    "We will use the bag-of-words model to represent each article.  \n",
    "\n",
    "There are a couple of variations of that model: term frequency , which represents each document as a vector of word counts, also there is term frequency-inverse document frequency which is the same as tf except that each word is weighted by its significance to that article.  \n",
    "\n",
    "Luckily for us, scikit-learn has a class just for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "train_x = tfidf.fit_transform(twenty_train.data)# fit on training data\n",
    "train_y = twenty_train.target # train target values\n",
    "test_x = tfidf.transform(twenty_test.data) # transform test data to tfidf representation\n",
    "test_y = twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for training\n",
    "When doing classification using neural networks, we must have an output layer with k neurons where k is the total number of classes. If an instance belongs to a certain class, the output value for the corresponding neuron for that class should be 1 and all other values should be zero. Thus to be able to train our neural network, we have to transform the class labels of the articles to a vector having 1 at the correspoing label and 0 at all other positions. This type of vectors is known as one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming target classes into one-hot vectors\n",
    "def vector_to_one_hot(vector,no_classes):\n",
    "    vector=vector.astype(np.int32)\n",
    "    m = np.zeros((vector.shape[0], no_classes))\n",
    "    m[np.arange(vector.shape[0]), vector]=1\n",
    "    return m\n",
    "\n",
    "train_y =vector_to_one_hot(train_y,no_classes)\n",
    "test_y = vector_to_one_hot(test_y, no_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 500\n",
    "batch_size = 16\n",
    "display_step = 100\n",
    "\n",
    "beta = 1 # regularization parameter\n",
    "# Network Parameters\n",
    "n_hidden_1 = 8 # size of 1st hidden layer\n",
    "\n",
    "num_input = train_x.shape[1] #input vector size\n",
    "num_classes = no_classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining input and output\n",
    "\n",
    "With tensorflow, inputs and outputs to network are defined as placeholders.  \n",
    "The training data is the fed to these placeholder at training time.  \n",
    "Note we define the first dimension to be None meaning we don't have a fixed input size (batch size) and tensorflow will deduce that at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, num_input]) # place holder for nn input\n",
    "Y = tf.placeholder(\"float\", [None, num_classes]) # place holder for nn output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining weights\n",
    "Since our network has only one layer (you can add layer if you want), we will have two sets of weights : From input layer to the hidden layer and from the hidden layer to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1' : tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1,num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tie everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net (X):\n",
    "    layer_1 = tf.add(tf.matmul(X, weights['h1']),biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    out_layer = tf.add(tf.matmul(layer_1,weights['out']), biases['out'])\n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our loss function to be cross entropy with softmax probabilities. Tensorflow has a function to compute that. \n",
    "The idea of using cross entropy is to maximize the probability of the correct class. So by minimizing the loss (negative log probability), we are maximizing the probabilty of the correct label.  \n",
    "Optimization is done by means of the ADAM optimization method which is somehow more efficient than SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = neural_net(X)\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y))\n",
    "\n",
    "loss = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "#evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "y_pred = tf.argmax(logits,1)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get less training time, we divide training data into batches and perform all weights updates for all instances in the batch at one.  \n",
    "\n",
    "This method returns the next batch of given training data with the specified batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_batch(batch_size,train_x,train_y):\n",
    "    global train_index\n",
    "    \n",
    "    if train_index + batch_size >= train_x.shape[0]:\n",
    "        train_index += batch_size\n",
    "        return train_x[train_index:,:],train_y[train_index:,:]# false to indicate no more training batches\n",
    "    else :\n",
    "        r= train_x[train_index:train_index+batch_size,:],train_y[train_index:train_index+batch_size,:]\n",
    "        train_index += batch_size\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.30 , accuracy = 0.38 , at step 10\n",
      "loss = 1.18 , accuracy = 0.88 , at step 20\n",
      "loss = 1.02 , accuracy = 0.75 , at step 30\n",
      "loss = 0.97 , accuracy = 0.88 , at step 40\n",
      "loss = 0.90 , accuracy = 0.88 , at step 50\n",
      "loss = 0.84 , accuracy = 0.94 , at step 60\n",
      "loss = 0.86 , accuracy = 0.88 , at step 70\n",
      "loss = 0.77 , accuracy = 1.00 , at step 80\n",
      "loss = 0.83 , accuracy = 0.94 , at step 90\n",
      "loss = 0.80 , accuracy = 1.00 , at step 100\n",
      "loss = 0.75 , accuracy = 1.00 , at step 110\n",
      "loss = 0.76 , accuracy = 1.00 , at step 120\n",
      "loss = 0.78 , accuracy = 0.94 , at step 130\n",
      "loss = 0.84 , accuracy = 0.88 , at step 140\n",
      "done optimization\n",
      "Testing Accuracy: 0.914867\n",
      "f1 score :  0.914819748906\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    train_index=0\n",
    "    moreTrain = True\n",
    "    step = 0\n",
    "    while True:\n",
    "        step+=1\n",
    "        if train_index >= train_x.shape[0] : break\n",
    "        batch_x , batch_y = get_train_batch(batch_size,train_x.todense(),train_y)\n",
    "        sess.run(train_step, feed_dict={X:batch_x,Y:batch_y})\n",
    "        \n",
    "        if step % 10 == 0 :\n",
    "            \n",
    "            cur_loss,cur_accuracy = sess.run([loss,accuracy],feed_dict={X:batch_x,Y:batch_y}) \n",
    "            print ('loss = %.2f , accuracy = %.2f , at step %d' %(cur_loss, cur_accuracy,step))\n",
    "    \n",
    "\n",
    "    print (\"done optimization\")\n",
    "    y_p = sess.run(y_pred,feed_dict={X:test_x.todense(),\n",
    "                                      Y:test_y})\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_x.todense(),\n",
    "                                      Y: test_y}))\n",
    "    print (\"f1 score : \", \n",
    "          metrics.f1_score(twenty_test.target, y_p,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have a very good accuracy and f1-score on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(train_x,twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Testing accuracy :  0.91041931385\n",
      "SVM F1-score :  0.910699838044\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(test_x)\n",
    "print ('SVM Testing accuracy : ', metrics.accuracy_score(twenty_test.target,y_pred))\n",
    "print ('SVM F1-score : ', metrics.f1_score(twenty_test.target,y_pred,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the SVM slightly outperforms our neural network.  \n",
    "I strongly suggest You try on your own to play a little with the neural network layer structure and hyperparatmers and try to improve its performance. You can also add regularization and see how it goes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion\n",
    "In this tutorial we applied a simple neural network model on text classification. We represented our articles using TF-IDF vector space represenation. We then used cross entropy as our loss function. We trained the model and got very good accuracy and f1-score. We also tried and SVM model on the data and compared perfomance between the two models.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
